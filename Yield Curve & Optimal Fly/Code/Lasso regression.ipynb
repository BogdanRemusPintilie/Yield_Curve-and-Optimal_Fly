{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.33368488950587677\n",
      "R^2 Score: 0.9143551160905226\n",
      "Best alpha: 0.01\n",
      "Best cross-validated score: 0.9183872185003233\n",
      "\n",
      "Intercept (b0): -0.1643981614217566\n",
      "Coefficient for FF rate (b_FF rate): -0.5608306869611592\n",
      "Coefficient for 5-Year, 5-Year Forward Inflation Expectation Rate (b_5-Year, 5-Year Forward Inflation Expectation Rate): -0.2958926959114346\n",
      "Coefficient for Moody's Seasoned Aaa Corporate Bond Yield (b_Moody's Seasoned Aaa Corporate Bond Yield): -0.19022807841888137\n",
      "Coefficient for Liabilities and Capital: Liabilities: Deposits with F.R. Banks, Other Than Reserve Balances: U.S. Treasury, General Account: Week Average (b_Liabilities and Capital: Liabilities: Deposits with F.R. Banks, Other Than Reserve Balances: U.S. Treasury, General Account: Week Average): -0.18330725731546454\n",
      "Coefficient for 30-Year Fixed Rate Mortgage Average in the US (b_30-Year Fixed Rate Mortgage Average in the US): -0.029207025294657633\n",
      "Coefficient for Inflation (b_Inflation): 0.014816278194738533\n",
      "\n",
      "Mathematical formula for PC1:\n",
      "PC1 = -0.1644 + (-0.5608) * FF rate + (-0.2959) * 5-Year, 5-Year Forward Inflation Expectation Rate + (-0.1902) * Moody's Seasoned Aaa Corporate Bond Yield + (-0.1833) * Liabilities and Capital: Liabilities: Deposits with F.R. Banks, Other Than Reserve Balances: U.S. Treasury, General Account: Week Average + (-0.0292) * 30-Year Fixed Rate Mortgage Average in the US + (0.0148) * Inflation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bogdan\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.746e-03, tolerance: 1.672e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Bogdan\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e-03, tolerance: 1.751e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Bogdan\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.588e-03, tolerance: 2.242e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Bogdan\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.402e-03, tolerance: 2.025e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Bogdan\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.869e-03, tolerance: 2.057e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_excel(\"C:\\\\Users\\\\Bogdan\\\\OneDrive - University of Warwick\\\\Desktop\\\\Projects\\\\Yield Curve & Optimal Fly\\\\Data\\\\Regress data.xlsx\")\n",
    "\n",
    "X = df.drop(columns=['PC1', 'PC2', 'PC3', 'Date']) #just exogenous variables\n",
    "y = df['PC1']  # The target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#random_state is to get the same test training sets irrespective of how many runs u do and 42 doesn't stand for anything logical\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "y_train, y_test = train_test_split(y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler() #Basically what happens is you subtract the mean then divide by the std. Is is simple standardisation like the one in PCA. And the values that you get once you standardise cause they not the same right are calle z-score(s) and have a mean=0 and std=1\n",
    "X_train_scaled = scaler.fit_transform(X_train) #fit_transform just applies the scaler to the data like each feature/column.\n",
    "X_test_scaled = scaler.transform(X_test) #here fit is missing because I need the fit to be the same as the one in the training so the transform applies the fit from the training to the data like see fit_transform as 2 separate methods (fit calcualtes the mean and std of each column and transform applies the standardisation)\n",
    "\n",
    "# Fit the Ridge Regression Model\n",
    "lasso_reg = Lasso(alpha=0.1)  # ridge regression becasue of the multicollinearity (when predictors are highly correlated) i mean ridge with alpha=0 is the same as OLS but with alpha you have a greater penlaty for high coeff. Like OLS chooses the coef s.t. it minmises a cost function, that alpha is timed by the sum of coeff and added to the cost function so the coeff will be smaller once u do that and that helps with avoiding overfitting (although this is ehh cause it's not blac and withe as in waht is best to have). so OLS's cost function is the sume of squared residuals (actual - predicted) so it is prone to overfit.\n",
    "lasso_reg.fit(X_train_scaled, y_train) #the model learns the relationships between the input features (X_train_scaled) and the target variable (y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lasso_reg.predict(X_test_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2_score = lasso_reg.score(X_test_scaled, y_test) #this is r squared (this is for the model that was created in the training stage and now used test data) So ideally u want 1. Now if the test data is like 1 day then you can think that you overfit but if it is 50% of the data then u probably did smth pretty good. hope u got the idea with overfit but again if it works well during test and the test is sufficintly large then gg\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse}') #so on avergae I am missing by 0.83\n",
    "print(f'R^2 Score: {r2_score}') #again like this a debate of waht a good R**2 value is fora 20% test data and a ridge regress\n",
    "\n",
    "#Tuning the alpha hyperparameter using GridSearchCV\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "ridge_cv = GridSearchCV(Lasso(), alpha_values, cv=5) # cv=5 = five fold cross validation. The model is trained on 4 subsets and validated on the remaining one. This process is repeated 5 times (each time with a different validation subset), and the results are averaged to assess the model's performance.\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f'Best alpha: {ridge_cv.best_params_[\"alpha\"]}') #retrieves the best alpha\n",
    "print(f'Best cross-validated score: {ridge_cv.best_score_}') #like best R**2 using th ebest alpha\n",
    "print('')\n",
    "\n",
    "#either way u spin it you have to choose a test size (%) and an alpha based on other aspects and not grid search cause drig search will make alpha 0 and test data small so unless u get better R^2 values for higher alpha and higher test then grid is illogical and the test_data split is pretty much depnedent on your horizon, alph is for how much u wanna avoid overfitting\n",
    "\n",
    "# Extract the coefficients and intercept\n",
    "coefficients = lasso_reg.coef_\n",
    "intercept = lasso_reg.intercept_\n",
    "\n",
    "# Create a DataFrame to store features and their corresponding coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Abs_Coefficient': np.abs(coefficients)  # Take the absolute value of the coefficients\n",
    "})\n",
    "\n",
    "# Filter out features where the absolute value of the coefficient is 0\n",
    "coef_df = coef_df[coef_df['Abs_Coefficient'] > 0]\n",
    "\n",
    "# Sort the DataFrame by the absolute value of the coefficients in descending order\n",
    "coef_df = coef_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Display the intercept\n",
    "print(\"Intercept (b0):\", intercept)\n",
    "\n",
    "# Display the non-zero coefficients\n",
    "for _, row in coef_df.iterrows():\n",
    "    print(f\"Coefficient for {row['Feature']} (b_{row['Feature']}): {row['Coefficient']}\")\n",
    "\n",
    "# Create the mathematical formula only with non-zero coefficients\n",
    "formula = \"PC1 = \" + f\"{intercept:.4f}\"\n",
    "for _, row in coef_df.iterrows():\n",
    "    formula += f\" + ({row['Coefficient']:.4f}) * {row['Feature']}\"\n",
    "\n",
    "print(\"\\nMathematical formula for PC1:\")\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.49219778388230434\n",
      "R^2 Score: 0.6895223159401789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bogdan\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.002e-04, tolerance: 4.080e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Bogdan\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.386e-04, tolerance: 4.268e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Bogdan\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.299e-04, tolerance: 3.451e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.001\n",
      "Best cross-validated score: 0.5753329522889647\n",
      "\n",
      "Intercept (b0): -0.07462830646157541\n",
      "Coefficient for 30-year Breakeven Inflation Rate (%) (b_30-year Breakeven Inflation Rate (%)): 0.19013424623357297\n",
      "Coefficient for Personal Saving Rate (b_Personal Saving Rate): -0.18948485830345213\n",
      "Coefficient for 7-year Breakeven Inflation Rate (%) (b_7-year Breakeven Inflation Rate (%)): 0.12998447530541354\n",
      "\n",
      "Mathematical formula for PC2:\n",
      "PC2 = -0.0746 + (0.1901) * 30-year Breakeven Inflation Rate (%) + (-0.1895) * Personal Saving Rate + (0.1300) * 7-year Breakeven Inflation Rate (%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_excel(\"C:\\\\Users\\\\Bogdan\\\\OneDrive - University of Warwick\\\\Desktop\\\\Projects\\\\Yield Curve & Optimal Fly\\\\Data\\\\Regress data.xlsx\")\n",
    "\n",
    "X = df.drop(columns=['PC1', 'PC2', 'PC3', 'Date']) #just exogenous variables\n",
    "y = df['PC2']  # The target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#random_state is to get the same test training sets irrespective of how many runs u do and 42 doesn't stand for anything logical\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "y_train, y_test = train_test_split(y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler() #Basically what happens is you subtract the mean then divide by the std. Is is simple standardisation like the one in PCA. And the values that you get once you standardise cause they not the same right are calle z-score(s) and have a mean=0 and std=1\n",
    "X_train_scaled = scaler.fit_transform(X_train) #fit_transform just applies the scaler to the data like each feature/column.\n",
    "X_test_scaled = scaler.transform(X_test) #here fit is missing because I need the fit to be the same as the one in the training so the transform applies the fit from the training to the data like see fit_transform as 2 separate methods (fit calcualtes the mean and std of each column and transform applies the standardisation)\n",
    "\n",
    "# Fit the Ridge Regression Model\n",
    "lasso_reg = Lasso(alpha=0.1)  # ridge regression becasue of the multicollinearity (when predictors are highly correlated) i mean ridge with alpha=0 is the same as OLS but with alpha you have a greater penlaty for high coeff. Like OLS chooses the coef s.t. it minmises a cost function, that alpha is timed by the sum of coeff and added to the cost function so the coeff will be smaller once u do that and that helps with avoiding overfitting (although this is ehh cause it's not blac and withe as in waht is best to have). so OLS's cost function is the sume of squared residuals (actual - predicted) so it is prone to overfit.\n",
    "lasso_reg.fit(X_train_scaled, y_train) #the model learns the relationships between the input features (X_train_scaled) and the target variable (y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lasso_reg.predict(X_test_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2_score = lasso_reg.score(X_test_scaled, y_test) #this is r squared (this is for the model that was created in the training stage and now used test data) So ideally u want 1. Now if the test data is like 1 day then you can think that you overfit but if it is 50% of the data then u probably did smth pretty good. hope u got the idea with overfit but again if it works well during test and the test is sufficintly large then gg\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse}') #so on avergae I am missing by 0.83\n",
    "print(f'R^2 Score: {r2_score}') #again like this a debate of waht a good R**2 value is fora 20% test data and a ridge regress\n",
    "\n",
    "#Tuning the alpha hyperparameter using GridSearchCV\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "ridge_cv = GridSearchCV(Lasso(), alpha_values, cv=5) # cv=5 = five fold cross validation. The model is trained on 4 subsets and validated on the remaining one. This process is repeated 5 times (each time with a different validation subset), and the results are averaged to assess the model's performance.\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f'Best alpha: {ridge_cv.best_params_[\"alpha\"]}') #retrieves the best alpha\n",
    "print(f'Best cross-validated score: {ridge_cv.best_score_}') #like best R**2 using th ebest alpha\n",
    "print('')\n",
    "\n",
    "#either way u spin it you have to choose a test size (%) and an alpha based on other aspects and not grid search cause drig search will make alpha 0 and test data small so unless u get better R^2 values for higher alpha and higher test then grid is illogical and the test_data split is pretty much depnedent on your horizon, alph is for how much u wanna avoid overfitting\n",
    "\n",
    "# Extract the coefficients and intercept\n",
    "coefficients = lasso_reg.coef_\n",
    "intercept = lasso_reg.intercept_\n",
    "\n",
    "# Create a DataFrame to store features and their corresponding coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Abs_Coefficient': np.abs(coefficients)  # Take the absolute value of the coefficients\n",
    "})\n",
    "\n",
    "# Filter out features where the absolute value of the coefficient is 0\n",
    "coef_df = coef_df[coef_df['Abs_Coefficient'] > 0]\n",
    "\n",
    "# Sort the DataFrame by the absolute value of the coefficients in descending order\n",
    "coef_df = coef_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Display the intercept\n",
    "print(\"Intercept (b0):\", intercept)\n",
    "\n",
    "# Display the non-zero coefficients\n",
    "for _, row in coef_df.iterrows():\n",
    "    print(f\"Coefficient for {row['Feature']} (b_{row['Feature']}): {row['Coefficient']}\")\n",
    "\n",
    "# Create the mathematical formula only with non-zero coefficients\n",
    "formula = \"PC2 = \" + f\"{intercept:.4f}\"\n",
    "for _, row in coef_df.iterrows():\n",
    "    formula += f\" + ({row['Coefficient']:.4f}) * {row['Feature']}\"\n",
    "\n",
    "print(\"\\nMathematical formula for PC2:\")\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bogdan\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.614e-04, tolerance: 6.372e-05\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Bogdan\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.859e-05, tolerance: 3.974e-05\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Bogdan\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.733e-04, tolerance: 5.922e-05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.3208470096235788\n",
      "R^2 Score: -0.1817462762859885\n",
      "Best alpha: 0.1\n",
      "Best cross-validated score: -1.1711233090892499\n",
      "\n",
      "Intercept (b0): 0.022213107834411133\n",
      "Coefficient for Median Sales Price of Houses Sold for the US (b_Median Sales Price of Houses Sold for the US): 0.032039300941968635\n",
      "Coefficient for Reserves of Depository Institutions: Total (b_Reserves of Depository Institutions: Total): -0.009471282309944167\n",
      "\n",
      "Mathematical formula for PC3:\n",
      "PC3 = 0.0222 + (0.0320) * Median Sales Price of Houses Sold for the US + (-0.0095) * Reserves of Depository Institutions: Total\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_excel(\"C:\\\\Users\\\\Bogdan\\\\OneDrive - University of Warwick\\\\Desktop\\\\Projects\\\\Yield Curve & Optimal Fly\\\\Data\\\\Regress data.xlsx\")\n",
    "\n",
    "X = df.drop(columns=['PC1', 'PC2', 'PC3', 'Date']) #just exogenous variables\n",
    "y = df['PC3']  # The target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#random_state is to get the same test training sets irrespective of how many runs u do and 42 doesn't stand for anything logical\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "y_train, y_test = train_test_split(y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler() #Basically what happens is you subtract the mean then divide by the std. Is is simple standardisation like the one in PCA. And the values that you get once you standardise cause they not the same right are calle z-score(s) and have a mean=0 and std=1\n",
    "X_train_scaled = scaler.fit_transform(X_train) #fit_transform just applies the scaler to the data like each feature/column.\n",
    "X_test_scaled = scaler.transform(X_test) #here fit is missing because I need the fit to be the same as the one in the training so the transform applies the fit from the training to the data like see fit_transform as 2 separate methods (fit calcualtes the mean and std of each column and transform applies the standardisation)\n",
    "\n",
    "# Fit the Ridge Regression Model\n",
    "lasso_reg = Lasso(alpha=0.1)  # ridge regression becasue of the multicollinearity (when predictors are highly correlated) i mean ridge with alpha=0 is the same as OLS but with alpha you have a greater penlaty for high coeff. Like OLS chooses the coef s.t. it minmises a cost function, that alpha is timed by the sum of coeff and added to the cost function so the coeff will be smaller once u do that and that helps with avoiding overfitting (although this is ehh cause it's not blac and withe as in waht is best to have). so OLS's cost function is the sume of squared residuals (actual - predicted) so it is prone to overfit.\n",
    "lasso_reg.fit(X_train_scaled, y_train) #the model learns the relationships between the input features (X_train_scaled) and the target variable (y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lasso_reg.predict(X_test_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2_score = lasso_reg.score(X_test_scaled, y_test) #this is r squared (this is for the model that was created in the training stage and now used test data) So ideally u want 1. Now if the test data is like 1 day then you can think that you overfit but if it is 50% of the data then u probably did smth pretty good. hope u got the idea with overfit but again if it works well during test and the test is sufficintly large then gg\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse}') #so on avergae I am missing by 0.83\n",
    "print(f'R^2 Score: {r2_score}') #again like this a debate of waht a good R**2 value is fora 20% test data and a ridge regress\n",
    "\n",
    "#Tuning the alpha hyperparameter using GridSearchCV\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "ridge_cv = GridSearchCV(Lasso(), alpha_values, cv=5) # cv=5 = five fold cross validation. The model is trained on 4 subsets and validated on the remaining one. This process is repeated 5 times (each time with a different validation subset), and the results are averaged to assess the model's performance.\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f'Best alpha: {ridge_cv.best_params_[\"alpha\"]}') #retrieves the best alpha\n",
    "print(f'Best cross-validated score: {ridge_cv.best_score_}') #like best R**2 using th ebest alpha\n",
    "print('')\n",
    "\n",
    "#either way u spin it you have to choose a test size (%) and an alpha based on other aspects and not grid search cause drig search will make alpha 0 and test data small so unless u get better R^2 values for higher alpha and higher test then grid is illogical and the test_data split is pretty much depnedent on your horizon, alph is for how much u wanna avoid overfitting\n",
    "\n",
    "# Extract the coefficients and intercept\n",
    "coefficients = lasso_reg.coef_\n",
    "intercept = lasso_reg.intercept_\n",
    "\n",
    "# Create a DataFrame to store features and their corresponding coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Abs_Coefficient': np.abs(coefficients)  # Take the absolute value of the coefficients\n",
    "})\n",
    "\n",
    "# Filter out features where the absolute value of the coefficient is 0\n",
    "coef_df = coef_df[coef_df['Abs_Coefficient'] > 0]\n",
    "\n",
    "# Sort the DataFrame by the absolute value of the coefficients in descending order\n",
    "coef_df = coef_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Display the intercept\n",
    "print(\"Intercept (b0):\", intercept)\n",
    "\n",
    "# Display the non-zero coefficients\n",
    "for _, row in coef_df.iterrows():\n",
    "    print(f\"Coefficient for {row['Feature']} (b_{row['Feature']}): {row['Coefficient']}\")\n",
    "\n",
    "# Create the mathematical formula only with non-zero coefficients\n",
    "formula = \"PC3 = \" + f\"{intercept:.4f}\"\n",
    "for _, row in coef_df.iterrows():\n",
    "    formula += f\" + ({row['Coefficient']:.4f}) * {row['Feature']}\"\n",
    "\n",
    "print(\"\\nMathematical formula for PC3:\")\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC3 is not influenced by the macro indicators I compiled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
