{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edbbeef1-e986-474f-9712-59e985341ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a55dc1-fb91-4ab2-8353-b002f803e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Bogdan\\OneDrive - University of Warwick\\Desktop\\Projects\\(GOV BONDS) Yield Curve Arbitrage\\Data\\Expected values of predictor variables (6 mo horizon - 31-12-2024)\\Work\\Personal Saving Rate - NOT STATIONARY\\Data.csv\")\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df = df.set_index('DATE')\n",
    "name = \"PSAVERT\"\n",
    "\n",
    "data = df[name] # only care about this column\n",
    "dataset = np.array(data).reshape(-1,1)#convert the column to an array and this will be 1-D array # then you need to rehape to a 2-D array cause minMaxScaler takes 2-D arrays and for that use reshape(-1,1) cause you know you want 1 column and the rows are automaticall calculated to satisfy the 1 columns and beleiv it is just the shape[0]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) #this is called normalizing (an action that is performed on an array). Here you only specify the range.\n",
    "scaled_data = scaler.fit_transform(dataset) #maps values in the array to a values in the interval (0,1) while keeping the same distribution of dataset. Reason: MLs work best if all data is scaled similarly.\n",
    "train_data = scaled_data #this is the subset of my scaled data that I will use for taining the ML scaled_data[rows,columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c53f25c9-ed21-427c-b19e-f0afdbc53ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "window_size = 30 #this means I believe 12 sequential values are good enough for the ML to take in so as to predict the 13th\n",
    "for i in range(window_size, len(train_data)):\n",
    "    x_train.append(train_data[i-window_size:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "#print(x_train[:2])\n",
    "#print(y_train)\n",
    "#these are lists of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110bb2ca-7803-4880-ab0f-63d7b4ed3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train) #this is the array with the sequences of values that are used as input for training\n",
    "y_train = np.array(y_train) #this is the array with the predictions that are used for training\n",
    "#in a nutshell both arrays are used for training. x is the input and y is the output and the ML learns to mimic the process meaning given 12 values spit the 13th. And because here you don't actually predict anything since all values are known you use this to train the ML meaning the ML takes the data you just inputed as input and output for granted\n",
    "#the above are singular arrays ok\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1)) #this is an array of arrays #kinda intuitive that you need that because y_train is safe to be a singlular arry cause you match the 12 values to 1 prediction so that 1 prediction can be the first element of an array but the 12 values are the first element of an array too so that is why you need arrays of arrays because for x the \"first value\" is the 12 values which you gotta stuff into an array cause you need to understand that there are 12 values stuck in there\n",
    "#print(x_train) this is a 3-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e50dff23-55b1-4022-b021-65a8ad12e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bogdan\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0380\n",
      "Epoch 2/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0183\n",
      "Epoch 3/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0131\n",
      "Epoch 4/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0112\n",
      "Epoch 5/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0075\n",
      "Epoch 6/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0076 \n",
      "Epoch 7/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0045\n",
      "Epoch 8/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0149\n",
      "Epoch 9/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0064 \n",
      "Epoch 10/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0090\n",
      "Epoch 11/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0040\n",
      "Epoch 12/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0130\n",
      "Epoch 13/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0119\n",
      "Epoch 14/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0037\n",
      "Epoch 15/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0080\n",
      "Epoch 16/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0102\n",
      "Epoch 17/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0103\n",
      "Epoch 18/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0105\n",
      "Epoch 19/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0073\n",
      "Epoch 20/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0054\n",
      "Epoch 21/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0104\n",
      "Epoch 22/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0102\n",
      "Epoch 23/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0046\n",
      "Epoch 24/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0049\n",
      "Epoch 25/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0059\n",
      "Epoch 26/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0090\n",
      "Epoch 27/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0064\n",
      "Epoch 28/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0026\n",
      "Epoch 29/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0061\n",
      "Epoch 30/30\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24a6cfa6bd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(window_size, 1))) #50 is the number of units/neurons, return_sequences = True means this layer will return a sequence of vectors each containing the states of the neurons at one timestep of the input sequence that is of length window_size to the next layer  input_shape declares the structure of the input expected. here you have arrays of 12x1\n",
    "#in short this layer returns a sequence of 12/window_size vectors with each vector containing 50 elements/states representing states of the neurons\n",
    "#what happens is you feed the first value of the input_sequence(to be determined) to all neurons simultaneously then you save the state of all neurons in a vector (that is the very first vector containing 50 elements/states at timestep 1 which is the first value fed). Aterwards you feed the second value to all neurons, the states might change and save the states to the second vector and so on\n",
    "model.add(LSTM(50, return_sequences=False))#return_sequences=Flase means the layer will return the vector containing the neuron states at the last timestep\n",
    "model.add(Dense(25))#25 neurons all connected to the previous layer's neurons,  each neuron in dense does a weighted sum of the inputs it receives, output 25 values computed as a weighted sum of inputs.\n",
    "model.add(Dense(1))#1 neuron that will output a single value computed as a weighted sum of its inputs.\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error') #this configures the model to aim to minimize the loss function = MSE by adapting the weights assigned to parameters by the Adam algorithm\n",
    "model.fit(x_train, y_train, batch_size=8, epochs=30) #batch_size = 1 means that I want to take 1 input at a time so one of those 12x1 arrays at a time + model will update its weights after each sample epochs=1 means I want to go through all the x_train dataset once\n",
    "#also y_train is the results dataset that you calculate the predictions MSE against in the training stage. So the model will take the first array within x_train array, will start off by having random weights (input + recurrent) which will be adjusted to minimize the MSE. This process occurs for the number of batches and what you want to happen is to get to a point where the weights converge to a value\n",
    "#this ML model skips the validation stage (will be covered as I learn) this means we use the default settings of adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028d5739-73b1-4a89-9029-3bc1660139ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "            Predictions\n",
      "2024-08-01     3.730997\n",
      "2024-09-01     3.869255\n",
      "2024-10-01     4.047383\n",
      "2024-11-01     4.217264\n",
      "2024-12-01     4.373681\n",
      "2025-01-01     4.519139\n"
     ]
    }
   ],
   "source": [
    "window_size = 30\n",
    "input_seq = train_data[-window_size:]\n",
    "input_seq = np.reshape(input_seq, (1, window_size, 1))\n",
    "\n",
    "predicted_values = []\n",
    "for _ in range(6):  # Predict the next 6 months\n",
    "    pred = model.predict(input_seq)\n",
    "    predicted_values.append(pred[0,0])  # Assuming the output is [batch, timestep, feature]\n",
    "    \n",
    "    # Update input_seq to include the new prediction\n",
    "    input_seq = np.append(input_seq[:, 1:, :], np.reshape(pred, (1, 1, 1)), axis=1)\n",
    "\n",
    "# Reverse scaling to get actual values\n",
    "predicted_values = np.array(predicted_values).reshape(-1, 1)\n",
    "predicted_values = scaler.inverse_transform(predicted_values)\n",
    "\n",
    "# Prepare dates for the predictions\n",
    "last_date = data.index[-1]\n",
    "date_range = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=6, freq='MS')\n",
    "\n",
    "# Create a DataFrame for the predicted values\n",
    "prediction_df = pd.DataFrame(data=predicted_values, index=date_range, columns=['Predictions'])\n",
    "print(prediction_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
